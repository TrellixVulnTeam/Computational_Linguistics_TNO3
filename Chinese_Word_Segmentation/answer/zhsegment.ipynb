{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zhsegment: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from zhsegment import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签 订 高 科 技 合 作 协 议\n",
      "新 华 社 上 海 八 月 三 十 一 日 电 （ 记 者 白 国 良 、 夏 儒 阁 ）\n",
      "“ 中 美 合 作 高 科 技 项 目 签 字 仪 式 ” 今 天 在 上 海 举 行 。\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"))\n",
    "segmenter = Segment(Pw) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip()))\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.27\n"
     ]
    }
   ],
   "source": [
    "from zhsegment_check import fscore\n",
    "with open('../data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.2f}\".format(tally), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing initial baseline system of iterative segmenter \n",
    "#### Version 1 : baseline model\n",
    "Developing a segmenter using the pseudo-code provided that uses unigram probabilities to get close to the baseline system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class iterative_segmenter:\n",
    "    \"\"\" Data Structure of the iterative_segmenter\n",
    "    input: The input sequence of characters\n",
    "    chart: The dynamic programming table to store the argmax for \n",
    "           every prefix of input indexed by character position in input\n",
    "    Entry: Each entry in the chart has four components: (word, start-position, log-probability, back-pointer)\n",
    "           the back-pointer in each entry links to a previous entry that it extends\n",
    "    heap:  A list or priority queue containing the entries to be expanded, sorted on start-position or log-probability\n",
    "    \"\"\"\n",
    "    def __init__(self, Pw):\n",
    "        self.Pw = Pw # unigram probability distribution\n",
    "        self.heap = [] # choosing heap to be list instead of priority queue \n",
    "        \n",
    "    def find_keys_matching_string(self, phrase):\n",
    "        \"\"\"phrase is a string which will be searched\"\"\" \n",
    "        keys_matching_string = []\n",
    "        for i in range(len(phrase)):\n",
    "            if Pw(phrase[0:i+1])[0]: # there is a matching word in unigram!\n",
    "                keys_matching_string.append(phrase[0:i+1])\n",
    "        return keys_matching_string\n",
    "\n",
    "    def initialize_heap(self, input):\n",
    "        c0 = input[0] # get the first character of the input\n",
    "        starting_words = self.find_keys_matching_string(input)\n",
    "        if starting_words:\n",
    "            for word in starting_words:\n",
    "                self.heap.append( (word, 0, log10(Pw(word)[1]), None) )\n",
    "        else: # no starting words\n",
    "            self.heap.append( (input[0], 0, log10(Pw(input[0])[1]), None) )\n",
    "\n",
    "    def retrieve_from_nested_entry(self, nested_entry):\n",
    "        \"Assume nested_entry has same structure as Entry\"\n",
    "        if nested_entry[3]: # back_pointer not None \n",
    "            receive = self.retrieve_from_nested_entry(nested_entry[3])\n",
    "            return (receive + [nested_entry[0]])\n",
    "        else: # back_pointer None\n",
    "            return [nested_entry[0]]\n",
    "\n",
    "    def segment(self, input):\n",
    "        ## Initilize the heap ##\n",
    "        self.initialize_heap(input)\n",
    "        ## Iteratively fill in chart[i] for all i ##\n",
    "        self.chart = [None] * len(input) # initialize the chart\n",
    "        iterator = 0\n",
    "        while self.heap: # using fact the empty sequences are false\n",
    "            entry = self.heap.pop()\n",
    "            endindex = entry[1] + len(entry[0]) - 1\n",
    "            if endindex > len(input)-1:\n",
    "                continue # if the word goes out of input length\n",
    "            if self.chart[endindex] is not None: # there is preventry at endindex\n",
    "                if entry[2] > self.chart[endindex][2]: # entry has a higher probability than preventry\n",
    "                    self.chart[endindex] = entry\n",
    "                else: # entry has equal or lower probability than preventry\n",
    "                    continue # we have already found a good segmentation until endindex\n",
    "            else:\n",
    "                self.chart[endindex] = entry\n",
    "\n",
    "            # find newword which matches the input starting  at position endindex+1\n",
    "            target_index = endindex+1\n",
    "            if target_index > len(input)-1: # if target_index is output input range, skip!\n",
    "                continue\n",
    "            new_words = self.find_keys_matching_string(input[target_index:]) \n",
    "            if new_words: # not empty\n",
    "                for newword in new_words:\n",
    "                    new_entry = (newword, target_index, entry[2]+log10(Pw(newword)[1]), entry)\n",
    "                    if new_entry not in self.heap:\n",
    "                        self.heap.append(new_entry)\n",
    "            else: # empty, treat the unseen character as a word of length one\n",
    "                new_entry = (input[target_index:target_index+1], target_index, entry[2]+log10(Pw(input[target_index:target_index+1])[1]), entry)\n",
    "                self.heap.append(new_entry)\n",
    "        ## Get the best segmentation ##\n",
    "        final_index = len(input)-1\n",
    "        retrieving_entry = self.chart[final_index]\n",
    "        ret = self.retrieve_from_nested_entry(retrieving_entry)\n",
    "        \n",
    "        return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the baseline iterative segmenter output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签订 高 科技 合作 协议\n",
      "新华社 上海 八月 三十一日 电 （ 记者 白 国 良 、 夏儒阁 ）\n",
      "“ 中 美 合作 高 科技 项目 签字 仪式 ” 今天 在 上海 举行 。\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"))\n",
    "segmenter = iterative_segmenter(Pw) # updated iterative segmenter\n",
    "output_full = []\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip()))\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.87\n"
     ]
    }
   ],
   "source": [
    "from zhsegment_check import fscore\n",
    "with open('../data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.2f}\".format(tally), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving initial baseline system of iterative segmenter \n",
    "#### Version 2 : improved baseline model\n",
    "In the class iterative_segmenter, we originally started from length 1 (single character) to find the entry candidates to the maximum legnth of all the characters, and if the word does not exist in the probability distribution, we discarded the word as a candidate.\n",
    "In this version, we have relaxed the constraints such that we allowed the unseen words with length up to the length limits, LEN_LIMIT, to be the entry candidate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_keys_matching_string(self, phrase):\n",
    "        \"\"\"phrase is a string which will be searched\"\"\" \n",
    "        keys_matching_string = []\n",
    "        LEN_LIMIT = 4\n",
    "        for i in range(len(phrase)):\n",
    "            if Pw(phrase[0:i+1])[0]: \n",
    "                keys_matching_string.append(phrase[0:i+1])\n",
    "            elif i <= LEN_LIMIT:\n",
    "                keys_matching_string.append(phrase[0:i+1])\n",
    "            else:\n",
    "                break\n",
    "        return keys_matching_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply allowing the unseen words to be the candidate does not increase the F-score but makes it worse. Because the unigram probability for unseen word set to 1/N. Which implies that P(unseen_4_letter_word) == P(unseen_1_letter_word). For such case, the segmenter prefers the longer unseen words over short unseen words. This behaviour is counter intuitive because it much more probable for 4 character words then 10 character words to be in a sentence. \n",
    "\n",
    "To address this issue we also modify the missingfn in the class Pdist such that the probability of long chinese words decreases dramatically as the number of characters increases, thus the denominator of the missingfn has been modified with N*1000^len(k) as the probability of the missing words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pdist(dict):\n",
    "    \"A probability distribution estimated from counts in datafile.\"\n",
    "    def __init__(self, data=[], N=None, missingfn=None):\n",
    "        for key,count in data:\n",
    "            self[key] = self.get(key, 0) + int(count)\n",
    "        self.N = float(N or sum(self.values()))\n",
    "        self.missingfn = missingfn or (lambda k, N: 1./(N*1000**len(k))) # Evaluate the baseline iterative segmenter outputmodified\n",
    "    def __call__(self, key):\n",
    "        if key in self: return (True, self[key]/self.N)  ## on call this unigram probability is calculated\n",
    "        else: return (False, self.missingfn(key, self.N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the improved baseline iterative segmenter output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签订 高 科技 合作 协议\n",
      "新华社 上海 八月 三十一日 电 （ 记者 白国良 、 夏儒阁 ）\n",
      "“ 中 美 合作 高 科技 项目 签字 仪式 ” 今天 在 上海 举行 。\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(\"../data/count_1w.txt\"))\n",
    "segmenter = iterative_segmenter(Pw) # updated iterative segmenter\n",
    "output_full = []\n",
    "with open(\"../data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip()))\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "score: 0.92\n"
     ]
    }
   ],
   "source": [
    "from zhsegment_check import fscore\n",
    "with open('../data/reference/dev.out', 'r') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"score: {:.2f}\".format(tally), file=sys.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By implementing the initial baseline system of iterative segmenter that uses the unigram probabilities, we had significant improvement on our F-score from 0.27 to 0.87.\n",
    "In addition to the baseline system, we have experimented with one another extension of the baseline by simply relaxing the constraints on the unseend words with length up to the length limit to be the entry candidate. We originally started from a single character to find the entry candidates to the maximum length of all the characters, and if the word does not exist in the probability distribution, we discarded the word as a candidate.\n",
    "\n",
    "But there was a issue with simply allowing the unseen words to be the candidate since it does not improve the F-score, rather it worsen the score. This issue was caused by the unigram probability for unseen word setting to 1/N, which implies that P(unseen_4_letter_word) == P(unseen_1_letter_word). For such case, the segmenter prefers the longer unseen words over short unseen words. This behaviour is counter intuitive because it much more probable for 4 character words then 10 character words to be in a sentence. \n",
    "\n",
    "To address this issue we also modify the missingfn in the class Pdist such that the probability of long chinese words decreases dramatically as the number of characters increases, thus the denominator of the missingfn has been modified with N*1000^len(k) as the probability of the missing words.\n",
    "\n",
    "Thus, we have improved our F-score to be 0.92 with this second modified version."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
