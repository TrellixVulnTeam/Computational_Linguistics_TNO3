{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lexsub: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join('data','glove.6B.100d.magnitude'))\n",
    "output = []\n",
    "with open(os.path.join('data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=27.89\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "with open(os.path.join('data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "Write some beautiful documentation of your program here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation of Retrofit Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we have used retrofitting method to train a word vector to predict the synonyms of a given word. Retrofitting utilizes two type of data to train the word embeddings: theÂ pretrained word embedding (GloVe for this exercise) and ontology file which provide undirected graphs of words connected to other words having similar context. The trained word embeddings should be located in similar vector space to other words in the undirected graph.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lexsub import *\n",
    "import os\n",
    "from lexsub_check import precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Retrofit __init__ function\n",
    "\n",
    "The constructor creates and initializes three important variables - vocabulary, wvecs, and ontology_dic. Vocabulary is a set of words that exist as the pretrained word embedding. Wvecs is the dictionary variable containing the pymagnitude vector and the corresponding key created for faster access to data. Lastly ontology_dic is the dictionary object that contains list of ontology words of the key.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, wvec_file, ontology_file, topn=10):\n",
    "    if os.getcwd().split('/')[-1] == 'answer':\n",
    "        self.path = os.path.dirname(os.getcwd())\n",
    "    else:\n",
    "        self.path = os.getcwd()\n",
    "\n",
    "    self.vocabulary = set()\n",
    "    self.wvecs = pymagnitude.Magnitude(wvec_file)\n",
    "    self.topn = topn\n",
    "\n",
    "    self.Beta = {} # hyperparameter matrix key: (key1, key2); value: number of times the pair appears in ontology\n",
    "    self.wvecs_dic = {}\n",
    "    self.Q = {}\n",
    "    for key, vector in self.wvecs:\n",
    "        self.vocabulary.add(key)\n",
    "        self.Q[key] = vector\n",
    "\n",
    "    # store text data in ontolgy file\n",
    "    self.ontology_dic = {}\n",
    "    for line in open(ontology_file, 'r'):\n",
    "        words = line.lower().strip().split()\n",
    "        target = words[0]\n",
    "        contexts = words[1:]\n",
    "\n",
    "        if words[0] not in self.ontology_dic:\n",
    "            self.ontology_dic[words[0]] = [word for word in words[1:]]\n",
    "        else:\n",
    "            self.ontology_dic[words[0]] = self.ontology_dic[words[0]] + [word for word in words[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function returns list of ontology words of the target word stored in the ontology_dic. If no key matches the target, the function simply returns nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_qjs(self, target):\n",
    "    if target in self.ontology_dic:\n",
    "        contexts = self.ontology_dic[target]\n",
    "        return contexts\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates the new values for the vector qi. The constants alpha and beta are a hyperparameter which can be changed to improve the performance of the model. In this implementation, beta is set as the number of occurence of target, ontology word pairs in the onotlogy document. For alpha, we have tried __a = 1__ which resulted dev.out = __44.55__ and __a = 2__ resulted the dev.out score of __46.10__. As we kept increseing the alpha values, as __a = 3__ resulted dev.out score __48.50__ , __a = 15__ resulted dev.out score __52.32__ and lastly __a = 20__ dev.out score __52.73__, we figured out that increasing the alpha value gave us the better dev.out score. Increased accuracy due to increasing alpha can be explained as with the \"quality\" of ontology document. As it will be shown in the Alternative Methods section, the alternative methods with increased usage of ontology, suffered from decreased model accuracy. Meaning ontology could be acting as a noise to the model. By increasing the alpha we increase the weight applied to the pretrained vector and prevent the noise from ontology from sabotaging the updated word embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_qi(self):\n",
    "    a = 20\n",
    "    B = 1\n",
    "    for index, target in enumerate(self.vocabulary): \n",
    "        qjs = self.find_qjs(target) \n",
    "        if qjs == None:\n",
    "            continue\n",
    "        else:     \n",
    "            new_qi = np.zeros(self.Q[target].shape)\n",
    "            count = 0\n",
    "            for context in qjs: \n",
    "                if context in self.Q:\n",
    "                    new_qi += B * self.Q[context] \n",
    "                    count += 1\n",
    "            new_qi = (new_qi + a * self.Q[target]) / (count + a)\n",
    "            self.Q[target] = new_qi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates a glove.6B.100d.retrofit.txt file and glove.6B.100d.retrofit.magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_retrofit_txt(self):\n",
    "    text_file_name = os.path.join(self.path, 'data', 'glove.6B.100d.retrofit.txt')\n",
    "    with open(text_file_name, 'w') as f:\n",
    "        for key, vector in self.Q.items():\n",
    "            line = key\n",
    "            for num in vector:\n",
    "                line = line + \" \" + str(num)\n",
    "            line = line + '\\n'\n",
    "            f.write(line)\n",
    "\n",
    "    destination_file_name = os.path.join(self.path, 'data', 'glove.6B.100d.retrofit.magnitude')\n",
    "    converter.convert(text_file_name, destination_file_name)\n",
    "\n",
    "    return destination_file_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function creates set of vocabularies and assign it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrofit(self):\n",
    "    for t in range(self.topn):\n",
    "        self.update_qi()\n",
    "    Q_pymag_dest = self.create_retrofit_txt()\n",
    "    self.Q_pymag = pymagnitude.Magnitude(Q_pymag_dest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output after retrofitting implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexsub = LexSub(os.path.join(os.path.dirname(os.getcwd()), 'data','glove.6B.100d.retrofit.magnitude'), os.path.join(os.path.dirname(os.getcwd()), 'data','lexicons','ppdb-xl.txt'))\n",
    "lexsub.retrofit()\n",
    "output = []\n",
    "with open(os.path.join(os.path.dirname(os.getcwd()), 'data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sides aside edge bottom under hand part close below away\n",
    "sides aside edge bottom under hand part close below away\n",
    "sides aside edge bottom under hand part close below away\n",
    "sides aside edge bottom under hand part close below away\n",
    "sides aside edge bottom under hand part close below away\n",
    "sides aside edge bottom under hand part close below away\n",
    "sides aside edge bottom under hand part close below away\n",
    "sides aside edge bottom under hand part close below away\n",
    "sides aside edge bottom under hand part close below away\n",
    "sides aside edge bottom under hand part close below away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.path.dirname(os.getcwd()), 'data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score=52.73"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Do some analysis of the results. What ideas did you try? What worked and what did not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main hurdle of retrofitting implementation was creating efficient algorithm. Since GloVe datasets are consisted with 400,000 word vectors of dimensionality 100, and each vector has find its âneighboursâ in to update itself ten times the running time of the program can differ dramatically depending on how it is implemented. After investing many hours and creating several implementations, the solution settled on utilizing python set to store the neighbouring context words for each target word. Also, there are subtle detail one should consider when one of the context word does not exist in the pertained vector. Our baseline implementation handles this issue by simply ignoring the vector and removing corresponding term during the calculation. \n",
    "We have tried running with different types of txt files in lexicons. As a result, we have implemented retrofitting to combine the information about word senses from ppdb-xl.txt in order to modify the default word vectors. After some tests, we find that the retrofitting with ppdb-xl.txt has the best result which was __52.73__ dev.out score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alternative Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although following methods did not improve the F-score of the solution (in fact they yielded lower F-score), these methods applies heuristic to the baseline model. These methods add the out-of-vocabulary handling feature to the baseline method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First alternative methods, solves the aforementioned non-existing context word case by utilizing pymagnitudeâs __âout-of-vocabularyâ__ feature. Pymagnitude generates vector for __out-of-vocabulary__ words by observing the similar words in the pre-trained vector such that word with similar spellings will be assigned with high similarity value. By including word embeddings of unseen word during the vector update process, the new calculated vector will incorporate more information of its context words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second alternative methods "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self, wvec_file, ontology_file, topn=10):\n",
    "    self.vocabulary = set()\n",
    "    self.wvecs = pymagnitude.Magnitude(wvec_file)\n",
    "    self.topn = topn\n",
    "    self.Q = {} # updated vectors for pre_existing_vectors \n",
    "    self.context_out_of_words = set()\n",
    "\n",
    "    self.Beta = {} # hyperparameter matrix key: (key1, key2); value: number of times the pair appears in ontology\n",
    "\n",
    "    for key, vector in self.wvecs: \n",
    "        self.vocabulary.add(key)\n",
    "        self.Q[key] = vector\n",
    "\n",
    "    # look through the ontology and check for out_of_words\n",
    "    num_ontology_line = 0\n",
    "    with open(ontology_file) as f:\n",
    "        for i, l in enumerate(f):\n",
    "            pass\n",
    "        num_ontology_line = i+1\n",
    "\n",
    "    self.out_of_word = {} #  dictionary containing key and vector of newly added vocabulary\n",
    "    self.ontology_dic = {} # dictionary containing context words of a target word\n",
    "\n",
    "    ont = open(ontology_file, 'r')\n",
    "    for l in range(num_ontology_line):\n",
    "        line = ont.readline(l)\n",
    "        words = line.lower().strip().split() # line in ontology\n",
    "        for target in words: \n",
    "            contexts = [word for word in words if word!=target]\n",
    "            if target not in self.vocabulary: # check if the word vector exists # there is no vector for this word\n",
    "                new_vec = np.zeros(self.wvecs.query(target).shape)\n",
    "                sum_count = 0 \n",
    "                for context in contexts:\n",
    "                    if context in self.wvecs:\n",
    "                        sum_count += 1\n",
    "                        new_vec += self.wvecs.query(target)\n",
    "                    else:\n",
    "                        self.context_out_of_words.add(context)\n",
    "                if sum_count > 0 :\n",
    "                    new_vec = new_vec / sum_count\n",
    "                    self.Q[target] = new_vec    \n",
    "                    self.vocabulary.add(target) \n",
    "            if target not in self.ontology_dic:\n",
    "                self.ontology_dic[target] = set(contexts)\n",
    "            else:\n",
    "                self.ontology_dic[target] |= set(contexts)\n",
    "            for context in contexts:\n",
    "                if frozenset([target, context]) in self.Beta: \n",
    "                    self.Beta[frozenset([target, context])] += 1\n",
    "                else:\n",
    "                    self.Beta[frozenset([target, context])] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Second method enhances the context word dictionary (ontology_dic in the source code) and attempts to create Beta weight dictionary where its key is a frozenset of two words and value is the number of occurrence of the word pair in the ontology file. \n",
    "With frozenset key, key with different order still yields same value making the order of key ineffective __(dic[frozenset(A,B)] == dic[frozenset(B,A)])__. In the baseline implementation, the code only treats the zeroth word of the ontology line as a target and saved all latter words as context words for faster computation. However the ontology line is an undirected, fully-connected graph. Hense, the second method creates accurate ontology dictionary which reflects the definition of the ontology file. Also, this method takes different approach for treating the out-of-vocabulary word. Instead of utilizing he pymagnitudeâs feature, it computes the average of its context wordâs vector embedding if the context exist as the pertained vector. This method worsened dev.out score as __27.00__ which is similar to the default solution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
